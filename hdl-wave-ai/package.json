{
  "publisher": "capp",
  "repository": {
    "type": "git",
    "url": "https://github.com/andrewcapatina/hdl-wave-ai"
  },
  "name": "hdl-wave-ai",
  "displayName": "HDL Wave AI",
  "description": "AI-assisted hardware verification using VaporView waveforms and local or cloud LLMs",
  "icon": "icon.png",
  "version": "0.0.3",
  "license": "AGPL-3.0-or-later",
  "engines": {
    "vscode": "^1.109.0"
  },
  "categories": [
    "Other"
  ],
  "extensionDependencies": [
    "lramseyer.vaporview"
  ],
  "activationEvents": [],
  "main": "./dist/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "hdl-wave-ai.openChat",
        "title": "HDL Wave AI: Open Chat"
      },
      {
        "command": "hdl-wave-ai.debug",
        "title": "HDL Wave AI: Debug VaporView State"
      }
    ],
    "configuration": {
      "title": "HDL Wave AI",
      "properties": {
        "hdlWaveAi.provider": {
          "type": "string",
          "enum": [
            "anthropic",
            "openai-compatible"
          ],
          "enumDescriptions": [
            "Use the Anthropic API (Claude)",
            "Use any OpenAI-compatible API (Ollama, NVIDIA NIM, vLLM, etc.)"
          ],
          "default": "anthropic",
          "description": "LLM provider to use for analysis"
        },
        "hdlWaveAi.anthropic.apiKey": {
          "type": "string",
          "default": "",
          "description": "Anthropic API key"
        },
        "hdlWaveAi.anthropic.model": {
          "type": "string",
          "default": "claude-sonnet-4-6",
          "description": "Anthropic model ID"
        },
        "hdlWaveAi.openaiCompatible.baseUrl": {
          "type": "string",
          "default": "http://localhost:11434/v1",
          "description": "Base URL for OpenAI-compatible API (default is Ollama)"
        },
        "hdlWaveAi.openaiCompatible.apiKey": {
          "type": "string",
          "default": "ollama",
          "description": "API key for OpenAI-compatible API (any string works for Ollama)"
        },
        "hdlWaveAi.openaiCompatible.model": {
          "type": "string",
          "default": "qwen2.5-coder:32b",
          "description": "Model name for OpenAI-compatible API"
        },
        "hdlWaveAi.waveform.defaultEndTime": {
          "type": "number",
          "default": 10000,
          "minimum": 100,
          "description": "Fallback simulation end time when no VaporView markers are set. Increase for longer simulations without markers."
        },
        "hdlWaveAi.waveform.sampleStepSize": {
          "type": "number",
          "default": 1,
          "description": "Time step size for waveform sampling (in simulation time units)"
        },
        "hdlWaveAi.waveform.maxTransitions": {
          "type": "number",
          "default": 300,
          "minimum": 50,
          "description": "Maximum number of signal transitions to send to the LLM. If the collected transitions exceed this, they are evenly sampled to preserve temporal spread. Increase for larger models with bigger context windows."
        },
        "hdlWaveAi.hdl.searchPaths": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "default": [],
          "description": "Additional absolute directory paths to search for HDL source files (e.g. your RTL directory outside the workspace)"
        },
        "hdlWaveAi.hdl.maxModules": {
          "type": "number",
          "default": 5,
          "minimum": 1,
          "description": "Maximum number of HDL modules to send to the LLM, ranked by relevance to tracked signals"
        },
        "hdlWaveAi.hdl.maxCharsPerModule": {
          "type": "number",
          "default": 4000,
          "minimum": 500,
          "description": "Maximum characters per module. Larger modules are truncated at this limit."
        },
        "hdlWaveAi.chat.maxHistory": {
          "type": "number",
          "default": 20,
          "minimum": 2,
          "description": "Maximum number of chat messages to keep in context when conversational mode is enabled."
        },
        "hdlWaveAi.chat.conversational": {
          "type": "boolean",
          "default": true,
          "description": "When true (default), prior exchanges are included up to maxHistory. When false, each query is stateless â€” exactly 2 messages sent per inquiry (system + user)."
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run package",
    "compile": "npm run check-types && npm run lint && node esbuild.js",
    "watch": "npm-run-all -p watch:*",
    "watch:esbuild": "node esbuild.js --watch",
    "watch:tsc": "tsc --noEmit --watch --project tsconfig.json",
    "package": "npm run check-types && npm run lint && node esbuild.js --production",
    "compile-tests": "tsc -p . --outDir out",
    "watch-tests": "tsc -p . -w --outDir out",
    "pretest": "npm run compile-tests && npm run compile && npm run lint",
    "check-types": "tsc --noEmit",
    "lint": "eslint src",
    "test": "vscode-test"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.39.0",
    "marked": "^17.0.3",
    "openai": "^4.96.0"
  },
  "devDependencies": {
    "@types/mocha": "^10.0.10",
    "@types/node": "22.x",
    "@types/vscode": "^1.109.0",
    "@vscode/test-cli": "^0.0.12",
    "@vscode/test-electron": "^2.5.2",
    "esbuild": "^0.27.2",
    "eslint": "^9.39.2",
    "npm-run-all": "^4.1.5",
    "typescript": "^5.9.3",
    "typescript-eslint": "^8.54.0"
  }
}
