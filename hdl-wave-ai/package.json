{
  "publisher": "capp",
  "repository": {
    "type": "git",
    "url": "https://github.com/andrewcapatina/hdl-wave-ai"
  },
  "name": "hdl-wave-ai",
  "displayName": "HDL Wave AI",
  "description": "AI-assisted hardware verification using VCD and FST waveforms with local or cloud LLMs. Features tool-use (RAG) mode for large designs and a standalone MCP server for integration with Claude Code, Claude Desktop, and other AI tools.",
  "icon": "icon.png",
  "version": "0.1.1",
  "license": "AGPL-3.0-or-later",
  "engines": {
    "vscode": "^1.109.0"
  },
  "categories": [
    "Other"
  ],
  "extensionDependencies": [
    "lramseyer.vaporview"
  ],
  "activationEvents": [],
  "main": "./dist/extension.js",
  "bin": {
    "hdl-wave-mcp": "./dist/mcp-server.js"
  },
  "contributes": {
    "commands": [
      {
        "command": "hdl-wave-ai.openChat",
        "title": "HDL Wave AI: Open Chat"
      },
      {
        "command": "hdl-wave-ai.openChatWithFile",
        "title": "HDL Wave AI: Analyze Waveform File…"
      },
      {
        "command": "hdl-wave-ai.debug",
        "title": "HDL Wave AI: Debug VaporView State"
      }
    ],
    "menus": {
      "explorer/context": [
        {
          "command": "hdl-wave-ai.openChatWithFile",
          "when": "resourceExtname == .fst || resourceExtname == .vcd",
          "group": "navigation"
        }
      ]
    },
    "configuration": {
      "title": "HDL Wave AI",
      "properties": {
        "hdlWaveAi.provider": {
          "type": "string",
          "enum": [
            "anthropic",
            "openai-compatible"
          ],
          "enumDescriptions": [
            "Use the Anthropic API (Claude)",
            "Use any OpenAI-compatible API (Ollama, NVIDIA NIM, vLLM, etc.)"
          ],
          "default": "anthropic",
          "description": "LLM provider to use for analysis"
        },
        "hdlWaveAi.anthropic.apiKey": {
          "type": "string",
          "default": "",
          "description": "Anthropic API key"
        },
        "hdlWaveAi.anthropic.model": {
          "type": "string",
          "default": "claude-sonnet-4-6",
          "description": "Anthropic model ID"
        },
        "hdlWaveAi.openaiCompatible.baseUrl": {
          "type": "string",
          "default": "http://localhost:11434/v1",
          "description": "Base URL for OpenAI-compatible API (default is Ollama)"
        },
        "hdlWaveAi.openaiCompatible.apiKey": {
          "type": "string",
          "default": "ollama",
          "description": "API key for OpenAI-compatible API (any string works for Ollama)"
        },
        "hdlWaveAi.openaiCompatible.model": {
          "type": "string",
          "default": "qwen2.5-coder:32b",
          "description": "Model name for OpenAI-compatible API"
        },
        "hdlWaveAi.waveform.defaultEndTime": {
          "type": "number",
          "default": 10000,
          "minimum": 100,
          "description": "Fallback simulation end time when no VaporView markers are set. Increase for longer simulations without markers."
        },
        "hdlWaveAi.waveform.sampleStepSize": {
          "type": "number",
          "default": 1,
          "description": "Time step size for waveform sampling (in simulation time units)"
        },
        "hdlWaveAi.waveform.maxTransitions": {
          "type": "number",
          "default": 300,
          "minimum": 50,
          "description": "Maximum number of signal transitions to send to the LLM. If the collected transitions exceed this, they are evenly sampled to preserve temporal spread. Increase for larger models with bigger context windows."
        },
        "hdlWaveAi.waveform.useToolMode": {
          "type": "boolean",
          "default": true,
          "description": "Use tool-calling (RAG) mode for waveform analysis. The LLM queries signal data on-demand via tools instead of receiving all transitions upfront. Requires a provider that supports function calling (OpenAI-compatible or Anthropic). Falls back to legacy mode if unsupported."
        },
        "hdlWaveAi.hdl.searchPaths": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "default": [],
          "description": "Additional absolute directory paths to search for HDL source files (e.g. your RTL directory outside the workspace)"
        },
        "hdlWaveAi.hdl.maxModules": {
          "type": "number",
          "default": 10,
          "minimum": 1,
          "description": "Maximum number of HDL modules to send to the LLM, ranked by relevance to tracked signals"
        },
        "hdlWaveAi.hdl.maxCharsPerModule": {
          "type": "number",
          "default": 4000,
          "minimum": 500,
          "description": "Maximum characters per module. Larger modules are truncated at this limit."
        },
        "hdlWaveAi.chat.maxHistory": {
          "type": "number",
          "default": 20,
          "minimum": 2,
          "description": "Maximum number of chat messages to keep in context when conversational mode is enabled."
        },
        "hdlWaveAi.chat.conversational": {
          "type": "boolean",
          "default": true,
          "description": "When true (default), prior exchanges are included up to maxHistory. When false, each query is stateless — exactly 2 messages sent per inquiry (system + user)."
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run package",
    "compile": "npm run check-types && npm run lint && node esbuild.js",
    "watch": "npm-run-all -p watch:*",
    "watch:esbuild": "node esbuild.js --watch",
    "watch:tsc": "tsc --noEmit --watch --project tsconfig.json",
    "package": "npm run check-types && npm run lint && node esbuild.js --production",
    "compile-tests": "tsc -p . --outDir out",
    "watch-tests": "tsc -p . -w --outDir out",
    "pretest": "npm run compile-tests && npm run compile && npm run lint",
    "check-types": "tsc --noEmit",
    "lint": "eslint src",
    "test": "vscode-test",
    "mcp": "node dist/mcp-server.js"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.39.0",
    "@modelcontextprotocol/sdk": "^1.26.0",
    "highlight.js": "^11.11.1",
    "marked": "^17.0.3",
    "marked-highlight": "^2.2.3",
    "openai": "^4.96.0",
    "zod": "^3.25.76"
  },
  "devDependencies": {
    "@types/mocha": "^10.0.10",
    "@types/node": "22.x",
    "@types/vscode": "^1.109.0",
    "@vscode/test-cli": "^0.0.12",
    "@vscode/test-electron": "^2.5.2",
    "esbuild": "^0.27.2",
    "eslint": "^9.39.2",
    "npm-run-all": "^4.1.5",
    "typescript": "^5.9.3",
    "typescript-eslint": "^8.54.0"
  }
}
